{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buisness Understanding\n",
    "**Stakeholder:** \n",
    "* Medical Device Company\n",
    "\n",
    "**Problem:** \n",
    "* The major issue is being unable to produce effective monitoring and treatment technologies for myocardial infarction (MI) survivors. Being able to predict myocardial complications is essential for advancing technologies in this background. MI can occur without complications or with complications that do not worsen the long-term prognosis. However about half of the patients in the acute and subacute periods have complications that lead to worsening of the disease and even death. Predicting complications of myocardial infarction is importnat in order to carry out the necessary preventive measures in upcoming developing medical devices that will keep those complication in mind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "**Source:**\n",
    "* [University of California Irvine (UCI) Machine Learning Repositories](https://archive.ics.uci.edu/) \n",
    "\n",
    "**Dataset:**\n",
    "* [Myocardial Infarction Complications](https://archive.ics.uci.edu/dataset/579/myocardial+infarction+complications)\n",
    "    * 1700 rows, 124 columns \n",
    "    * 8 potential Target columns, 116 Feature columns\n",
    "    * All the column names and defintions can be found in the [Column Descriptions](../data/column_descriptions.md) file\n",
    "\n",
    "**Targets (Myocardial Infarction Complications):**\n",
    "* FIBR_PREDS (Atrial Fibrillation) \n",
    "    * Life-threatening, irregular heartbeat caused by fast and irregular contractions in the upper chambers of the heart. Prevents the heart from pumping blood to the lower chamber of the heart\n",
    "* PREDS_TAH (Supraventricular Tachycardia)\n",
    "    * Irregular, fast, or erratic heartbeat that affects the heart's upper chambers. Its not usually serious and does not cause sudden death, heart damage, or heart attacks. In extreme cases that can result but its very unlikely.\n",
    "* JELUD_TAH (Ventricular Tachycardia)\n",
    "    * Irregular, fast, or erratic heartbeat that affects the heart's lower chambers. Can become life-threatening if the episode lasts longer than a few seconds also known as a sustained Ventricular Tachycardia. \n",
    "* FIBR_JELUD (Ventricular Fibrillation)\n",
    "    * Life-threatening, irregular heartbeat that affects the heart's ventricles. The lower heart chambers contract rapidly and in an uncoordinated manner. As a result, prevents the heart from pumping blood to the rest of the body.\n",
    "* A_V_BLOK (Third-degree AV block)\n",
    "    * Medical condition that occurs when there is a complete loss of communcation between the heart's atria and ventricles. In other words, electrical signals cannot pass from the atria to the ventricles.   \n",
    "* OTEK_LANC (Pulmonary edema)\n",
    "    * Life-threatening condition caused by fluid build up in the lungs. This fluid collects in the air sacs in the lungs, making it difficult to breathe.\n",
    "* RAZRIV (Myocardial rupture)\n",
    "    * Tear in the heart that occurs after a heart attack. Is life-threatening however is a rare complication of a heart attack.\n",
    "* DRESSLER (Dressler Syndrome)\n",
    "    * Inflammation of the sac (pericardium) surrounding the heart. Immune system response due to damage to heart tissue or the sac itself.\n",
    "\n",
    "**Data Types:**\n",
    "\n",
    "For this dataset almost all columns have already been numerically encoded for nominal and ordinal columns. There are columns that are numeric that represent realtime values that were taken. These need to be scaled so they don't dramatically impact the modeling results.\n",
    "\n",
    "Kept Continous Data: \n",
    "* ALT_BLOOD (Serum AIAT content)\n",
    "* AST_BLOOD (Serum AsAT Content)\n",
    "* L_BLOOD (White Blood Cell Count)\n",
    "* AGE (Age of patient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split data into a train-test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/data_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>INF_ANAM</th>\n",
       "      <th>STENOK_AN</th>\n",
       "      <th>FK_STENOK</th>\n",
       "      <th>IBS_POST</th>\n",
       "      <th>GB</th>\n",
       "      <th>SIM_GIPERT</th>\n",
       "      <th>DLIT_AG</th>\n",
       "      <th>...</th>\n",
       "      <th>JELUD_TAH</th>\n",
       "      <th>FIBR_JELUD</th>\n",
       "      <th>A_V_BLOK</th>\n",
       "      <th>OTEK_LANC</th>\n",
       "      <th>RAZRIV</th>\n",
       "      <th>DRESSLER</th>\n",
       "      <th>ZSN</th>\n",
       "      <th>REC_IM</th>\n",
       "      <th>P_IM_STEN</th>\n",
       "      <th>LET_IS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  AGE  SEX  INF_ANAM  STENOK_AN  FK_STENOK  IBS_POST  GB  SIM_GIPERT  \\\n",
       "0   2   55    1         1        0.0        0.0         0   0           0   \n",
       "1   6   64    1         0        1.0        2.0         1   0           0   \n",
       "2   7   70    1         1        1.0        2.0         1   2           0   \n",
       "3  10   77    0         2        0.0        0.0         0   3           0   \n",
       "4  11   71    1         0        0.0        0.0         0   0           0   \n",
       "\n",
       "   DLIT_AG  ...  JELUD_TAH  FIBR_JELUD  A_V_BLOK  OTEK_LANC  RAZRIV  DRESSLER  \\\n",
       "0      0.0  ...          0           0         0          0       0         0   \n",
       "1      0.0  ...          0           0         0          0       0         0   \n",
       "2      7.0  ...          0           0         0          0       0         0   \n",
       "3      6.0  ...          0           0         0          0       0         0   \n",
       "4      0.0  ...          0           0         0          0       0         0   \n",
       "\n",
       "   ZSN  REC_IM  P_IM_STEN  LET_IS  \n",
       "0    0       0          0       0  \n",
       "1    0       0          0       0  \n",
       "2    1       0          0       0  \n",
       "3    1       0          0       0  \n",
       "4    0       0          0       0  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X, y \n",
    "X = df.iloc[:, 1:-12] # Features\n",
    "y = df.iloc[:, -12:-1] # Targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIBR_PREDS\n",
      "PREDS_TAH\n",
      "JELUD_TAH\n",
      "FIBR_JELUD\n",
      "A_V_BLOK\n",
      "OTEK_LANC\n",
      "RAZRIV\n",
      "DRESSLER\n",
      "ZSN\n",
      "REC_IM\n",
      "P_IM_STEN\n"
     ]
    }
   ],
   "source": [
    "for col in y.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets standardize our contnous columns in our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our numeric columns that need to be normalized\n",
    "numeric_features = [\n",
    "    \"ALT_BLOOD\",\n",
    "    \"AST_BLOOD\",\n",
    "    \"L_BLOOD\",\n",
    "    \"AGE\"\n",
    "]\n",
    "\n",
    "X_train_numeric = X_train[numeric_features].copy()\n",
    "X_test_numeric = X_test[numeric_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate scaler \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit Scaler to continous training data\n",
    "scaler.fit(X_train_numeric)\n",
    "\n",
    "# Concatenate training data and testing data respectively\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_train_numeric), \n",
    "    index=X_train_numeric.index,\n",
    "    columns=X_train_numeric.columns\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test_numeric), \n",
    "    index=X_test_numeric.index,\n",
    "    columns=X_test_numeric.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the scaled data with the original training data and drop the columns that are not scaled\n",
    "\n",
    "# Drop old columns\n",
    "X_train.drop(columns=numeric_features, inplace=True)\n",
    "X_test.drop(columns=numeric_features, inplace=True)\n",
    "\n",
    "# Concatenate the scaled data with the original training data \n",
    "X_train_full = pd.concat([X_train, X_train_scaled], axis=1)\n",
    "X_test_full = pd.concat([X_test, X_test_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create baseline moedels with no modifications to there performances. The well known fact about this dataset is that there are class imbalances so the baseline moedels should perform poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Pipeline(steps=[('classifier',\n",
      "                 MultiOutputClassifier(estimator=LogisticRegression(C=1000000000000.0,\n",
      "                                                                    fit_intercept=False,\n",
      "                                                                    solver='liblinear')))])...\n",
      "Evaluating Pipeline(steps=[('classifier',\n",
      "                 MultiOutputClassifier(estimator=LogisticRegression(C=1000000000000.0,\n",
      "                                                                    fit_intercept=False,\n",
      "                                                                    solver='liblinear')))])...\n",
      "Log Loss:8.827703717533455\n",
      "\n",
      "========================================\n",
      "\n",
      "Pipeline(steps=[('classifier',\n",
      "                 MultiOutputClassifier(estimator=LogisticRegression(C=1000000000000.0,\n",
      "                                                                    fit_intercept=False,\n",
      "                                                                    solver='liblinear')))]) Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  FIBR_PREDS       0.45      0.16      0.23        32\n",
      "   PREDS_TAH       0.08      0.17      0.11         6\n",
      "   JELUD_TAH       0.00      0.00      0.00         8\n",
      "  FIBR_JELUD       0.00      0.00      0.00         8\n",
      "    A_V_BLOK       0.00      0.00      0.00         4\n",
      "   OTEK_LANC       0.29      0.13      0.18        31\n",
      "      RAZRIV       0.08      0.17      0.11         6\n",
      "    DRESSLER       0.00      0.00      0.00        14\n",
      "         ZSN       0.48      0.15      0.23        93\n",
      "      REC_IM       0.00      0.00      0.00        25\n",
      "   P_IM_STEN       0.11      0.03      0.05        29\n",
      "\n",
      "   micro avg       0.21      0.10      0.14       256\n",
      "   macro avg       0.14      0.07      0.08       256\n",
      "weighted avg       0.28      0.10      0.15       256\n",
      " samples avg       0.07      0.08      0.07       256\n",
      "\n",
      "\n",
      "========================================\n",
      "\n",
      "Training Pipeline(steps=[('classifier',\n",
      "                 MultiOutputClassifier(estimator=RandomForestClassifier()))])...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Pipeline(steps=[('classifier',\n",
      "                 MultiOutputClassifier(estimator=RandomForestClassifier()))])...\n",
      "Log Loss:2.9877128636728596\n",
      "\n",
      "========================================\n",
      "\n",
      "Pipeline(steps=[('classifier',\n",
      "                 MultiOutputClassifier(estimator=RandomForestClassifier()))]) Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  FIBR_PREDS       0.00      0.00      0.00        32\n",
      "   PREDS_TAH       0.00      0.00      0.00         6\n",
      "   JELUD_TAH       0.00      0.00      0.00         8\n",
      "  FIBR_JELUD       0.00      0.00      0.00         8\n",
      "    A_V_BLOK       0.00      0.00      0.00         4\n",
      "   OTEK_LANC       0.00      0.00      0.00        31\n",
      "      RAZRIV       0.00      0.00      0.00         6\n",
      "    DRESSLER       0.00      0.00      0.00        14\n",
      "         ZSN       0.68      0.16      0.26        93\n",
      "      REC_IM       0.00      0.00      0.00        25\n",
      "   P_IM_STEN       0.00      0.00      0.00        29\n",
      "\n",
      "   micro avg       0.68      0.06      0.11       256\n",
      "   macro avg       0.06      0.01      0.02       256\n",
      "weighted avg       0.25      0.06      0.09       256\n",
      " samples avg       0.05      0.04      0.04       256\n",
      "\n",
      "\n",
      "========================================\n",
      "\n",
      "Training Pipeline(steps=[('classifier',\n",
      "                 MultiOutputClassifier(estimator=KNeighborsClassifier()))])...\n",
      "Evaluating Pipeline(steps=[('classifier',\n",
      "                 MultiOutputClassifier(estimator=KNeighborsClassifier()))])...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss:3.7796129918452785\n",
      "\n",
      "========================================\n",
      "\n",
      "Pipeline(steps=[('classifier',\n",
      "                 MultiOutputClassifier(estimator=KNeighborsClassifier()))]) Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  FIBR_PREDS       0.00      0.00      0.00        32\n",
      "   PREDS_TAH       0.00      0.00      0.00         6\n",
      "   JELUD_TAH       0.00      0.00      0.00         8\n",
      "  FIBR_JELUD       0.00      0.00      0.00         8\n",
      "    A_V_BLOK       0.00      0.00      0.00         4\n",
      "   OTEK_LANC       0.00      0.00      0.00        31\n",
      "      RAZRIV       0.00      0.00      0.00         6\n",
      "    DRESSLER       0.00      0.00      0.00        14\n",
      "         ZSN       0.62      0.11      0.18        93\n",
      "      REC_IM       0.00      0.00      0.00        25\n",
      "   P_IM_STEN       0.00      0.00      0.00        29\n",
      "\n",
      "   micro avg       0.40      0.04      0.07       256\n",
      "   macro avg       0.06      0.01      0.02       256\n",
      "weighted avg       0.23      0.04      0.07       256\n",
      " samples avg       0.03      0.03      0.03       256\n",
      "\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline \n",
    "# Define the pipelines for each algorithm Baseline\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('classifier', MultiOutputClassifier(LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')))\n",
    "])\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('classifier', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])\n",
    "\n",
    "knn_pipeline = Pipeline([\n",
    "    ('classifier', MultiOutputClassifier(KNeighborsClassifier()))\n",
    "])\n",
    "\n",
    "# Fit and evaluate the each baseline pipeline\n",
    "pipelines = [logreg_pipeline, rf_pipeline, knn_pipeline]\n",
    "\n",
    "# Fit and evaluate each pipeline\n",
    "for i, pipeline in enumerate([logreg_pipeline, rf_pipeline, knn_pipeline]):\n",
    "    print(f\"Training {pipelines[i]}...\")\n",
    "    \n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train_full, y_train)\n",
    "\n",
    "    print(f\"Evaluating {pipelines[i]}...\")\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    predictions = pipeline.predict(X_test_full)\n",
    "\n",
    "    # Evaluate the pipeline's performance\n",
    "    loss = log_loss(y_test, predictions)\n",
    "    report = classification_report(y_test, predictions, target_names=y.columns)\n",
    "\n",
    "    print(f\"Log Loss:{loss}\")\n",
    "    print(\"\\n\" + \"=\" * 40 + \"\\n\")\n",
    "    print(f\"{pipelines[i]} Classification Report:\\n\", report)\n",
    "    print(\"\\n\" + \"=\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we did poorly. In every model we have very low recall (Sensitivity). This suggests our models can't predict actual positive cases. The log loss for all models are very high. \n",
    "\n",
    "The next step is to fix the class imbalance in our data because this is significantly influencing prediction capabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalance\n",
    "\n",
    "We cant oversample because some feature are just so rare that it can be 1 or 2. Undersampling is a possibility however it runs the risk of underfitting. \n",
    "\n",
    "We can do a custom resample strategy where we oversample then undersample. We would need to sort our targets since some are bianry and others arent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1203, 104)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The specified ratio required to remove samples from the minority class while trying to generate new samples. Please increase the ratio.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-197-f725e26a9b3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtarget_col\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbinary_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0msmote\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mX_train_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_resampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mlogreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mmodel_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_resampled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         self.sampling_strategy_ = check_sampling_strategy(\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampling_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\utils\\_validation.py\u001b[0m in \u001b[0;36mcheck_sampling_strategy\u001b[1;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[0;32m    578\u001b[0m         return OrderedDict(\n\u001b[0;32m    579\u001b[0m             sorted(\n\u001b[1;32m--> 580\u001b[1;33m                 _sampling_strategy_float(\n\u001b[0m\u001b[0;32m    581\u001b[0m                     \u001b[0msampling_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m                 ).items()\n",
      "\u001b[1;32mc:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\imblearn\\utils\\_validation.py\u001b[0m in \u001b[0;36m_sampling_strategy_float\u001b[1;34m(sampling_strategy, y, sampling_type)\u001b[0m\n\u001b[0;32m    400\u001b[0m         }\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msampling_strategy_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    403\u001b[0m                 \u001b[1;34m\"The specified ratio required to remove samples \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[1;34m\"from the minority class while trying to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The specified ratio required to remove samples from the minority class while trying to generate new samples. Please increase the ratio."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Identify which target columns are binary and which are multi-class\n",
    "# let's compare a few different ratios of minority class to majority class\n",
    "ratios = [0.25, 0.33, 0.5, 0.7, 1]\n",
    "names = ['0.25', '0.33','0.5','0.7','even']\n",
    "colors = sns.color_palette('Set2')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "binary_targets = [\n",
    "    \"FIBR_PREDS\",\n",
    "    \"PREDS_TAH\",\n",
    "    \"JELUD_TAH\",\n",
    "    \"FIBR_JELUD\",\n",
    "    \"A_V_BLOK\", \n",
    "    \"OTEK_LANC\",\n",
    "    \"RAZRIV\",\n",
    "    \"DRESSLER\",\n",
    "    \"ZSN\",\n",
    "    \"REC_IM\",\n",
    "    \"P_IM_STEN\"\n",
    "]\n",
    "\n",
    "# Initialize lists to store ROC curve data\n",
    "all_fpr = []\n",
    "all_tpr = []\n",
    "for n, ratio in enumerate(ratios):\n",
    "    # Initialize empty arrays to store individual ROC curve data\n",
    "    fpr_list = []\n",
    "    tpr_list = []\n",
    "    \n",
    "    # Loop through each binary target column\n",
    "    for target_col in binary_targets:\n",
    "        smote = SMOTE(sampling_strategy=ratio, random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train[target_col]) \n",
    "        logreg = LogisticRegression(fit_intercept=False, C=1e20, solver='lbfgs')\n",
    "        model_log = logreg.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "        # Predict\n",
    "        y_score = logreg.decision_function(X_test)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_test[target_col], y_score)\n",
    "        \n",
    "        # Append ROC curve data for this binary target column\n",
    "        fpr_list.append(fpr)\n",
    "        tpr_list.append(tpr)\n",
    "\n",
    "    # Compute micro-average ROC curve and AUC\n",
    "    micro_fpr = np.unique(np.concatenate([fpr for fpr in fpr_list]))\n",
    "    micro_tpr = np.zeros_like(micro_fpr)\n",
    "    \n",
    "    for i in range(len(binary_targets)):\n",
    "        micro_tpr += np.interp(micro_fpr, fpr_list[i], tpr_list[i])\n",
    "    \n",
    "    micro_tpr /= len(binary_targets)\n",
    "    \n",
    "    # Store ROC curve data\n",
    "    all_fpr.append(micro_fpr)\n",
    "    all_tpr.append(micro_tpr)\n",
    "    \n",
    "    # Compute AUC\n",
    "    roc_auc = auc(micro_fpr, micro_tpr)\n",
    "    \n",
    "    print('AUC for {}: {}'.format(names[n], roc_auc))\n",
    "    print('-------------------------------------------------------------------------------------')\n",
    "\n",
    "# Plot ROC curves for each ratio\n",
    "plt.figure(figsize=(10, 8))\n",
    "lw = 2\n",
    "\n",
    "for n in range(len(ratios)):\n",
    "    plt.plot(all_fpr[n], all_tpr[n], color=colors[n], lw=lw, label='ROC curve {}'.format(names[n]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Befotr SMOTE for FIBR_PREDS:\n",
      "0    1085\n",
      "1     118\n",
      "Name: FIBR_PREDS, dtype: int64\n",
      "__________________________________________________\n",
      "Class Distribution After SMOTE for FIBR_PREDS:\n",
      "0    816\n",
      "1    244\n",
      "Name: FIBR_PREDS, dtype: int64\n",
      "Class Distribution Befotr SMOTE for PREDS_TAH:\n",
      "0    1186\n",
      "1      17\n",
      "Name: PREDS_TAH, dtype: int64\n",
      "__________________________________________________\n",
      "Class Distribution After SMOTE for PREDS_TAH:\n",
      "0    891\n",
      "1    169\n",
      "Name: PREDS_TAH, dtype: int64\n",
      "Class Distribution Befotr SMOTE for JELUD_TAH:\n",
      "0    1170\n",
      "1      33\n",
      "Name: JELUD_TAH, dtype: int64\n",
      "__________________________________________________\n",
      "Class Distribution After SMOTE for JELUD_TAH:\n",
      "0    877\n",
      "1    183\n",
      "Name: JELUD_TAH, dtype: int64\n",
      "Class Distribution Befotr SMOTE for FIBR_JELUD:\n",
      "0    1151\n",
      "1      52\n",
      "Name: FIBR_JELUD, dtype: int64\n",
      "__________________________________________________\n",
      "Class Distribution After SMOTE for FIBR_JELUD:\n",
      "0    858\n",
      "1    202\n",
      "Name: FIBR_JELUD, dtype: int64\n",
      "Class Distribution Befotr SMOTE for A_V_BLOK:\n",
      "0    1166\n",
      "1      37\n",
      "Name: A_V_BLOK, dtype: int64\n",
      "__________________________________________________\n",
      "Class Distribution After SMOTE for A_V_BLOK:\n",
      "0    869\n",
      "1    191\n",
      "Name: A_V_BLOK, dtype: int64\n",
      "Class Distribution Befotr SMOTE for OTEK_LANCRAZRIV:\n",
      "0    1094\n",
      "1     109\n",
      "Name: OTEK_LANCRAZRIV, dtype: int64\n",
      "__________________________________________________\n",
      "Class Distribution After SMOTE for OTEK_LANCRAZRIV:\n",
      "0    824\n",
      "1    236\n",
      "Name: OTEK_LANCRAZRIV, dtype: int64\n",
      "Class Distribution Befotr SMOTE for DRESSLER:\n",
      "0    1168\n",
      "1      35\n",
      "Name: DRESSLER, dtype: int64\n",
      "__________________________________________________\n",
      "Class Distribution After SMOTE for DRESSLER:\n",
      "0    873\n",
      "1    187\n",
      "Name: DRESSLER, dtype: int64\n",
      "Class Distribution Befotr SMOTE for ZSN:\n",
      "0    1146\n",
      "1      57\n",
      "Name: ZSN, dtype: int64\n",
      "__________________________________________________\n",
      "Class Distribution After SMOTE for ZSN:\n",
      "0    859\n",
      "1    201\n",
      "Name: ZSN, dtype: int64\n",
      "Class Distribution Befotr SMOTE for REC_IM:\n",
      "0    908\n",
      "1    295\n",
      "Name: REC_IM, dtype: int64\n",
      "__________________________________________________\n",
      "Class Distribution After SMOTE for REC_IM:\n",
      "0.0    700\n",
      "1.0    210\n",
      "Name: REC_IM, dtype: int64\n",
      "Class Distribution Befotr SMOTE for P_IM_STEN:\n",
      "0    1087\n",
      "1     116\n",
      "Name: P_IM_STEN, dtype: int64\n",
      "__________________________________________________\n",
      "Class Distribution After SMOTE for P_IM_STEN:\n",
      "0.0    811\n",
      "1.0    243\n",
      "Name: P_IM_STEN, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the class distribution for each resampled target column\n",
    "# for target_col in binary_targets:\n",
    "#     target_col_name = f\"{target_col}\"\n",
    "#     resampled_class_distribution = resampled_df[target_col_name].value_counts()\n",
    "#     original_class_distribution = df[target_col_name].value_counts()\n",
    "    \n",
    "#     print(f\"Class Distribution Befotr SMOTE for {target_col}:\")\n",
    "#     print(original_class_distribution)\n",
    "#     print(\"__________________________________________________\")\n",
    "#     print(f\"Class Distribution After SMOTE for {target_col}:\")\n",
    "#     print(resampled_class_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets split the resampled data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_re = resampled_df.iloc[:, 1:-12] # Features\n",
    "# y_re = resampled_df.iloc[:, -12:-1] #Targets\n",
    "\n",
    "# X_train_re, X_test_re, y_train_re, y_test_re = train_test_split(X_re, y_re, test_size=.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Instantiate scaler \n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Fit Scaler to continous training data\n",
    "# scaler.fit(X_train_numeric)\n",
    "\n",
    "# # Concatenate training data and testing data respectively\n",
    "# X_train_scaled_re = pd.DataFrame(\n",
    "#     scaler.transform(X_train_numeric), \n",
    "#     index=X_train_numeric.index,\n",
    "#     columns=X_train_numeric.columns\n",
    "# )\n",
    "\n",
    "# X_test_scaled_re = pd.DataFrame(\n",
    "#     scaler.transform(X_test_numeric), \n",
    "#     index=X_test_numeric.index,\n",
    "#     columns=X_test_numeric.columns\n",
    "# )\n",
    "\n",
    "# # Concatenate the scaled data with the original training data and drop the columns that are not scaled\n",
    "\n",
    "# # Drop old columns\n",
    "# X_train_re.drop(columns=numeric_features, inplace=True)\n",
    "# X_test_re.drop(columns=numeric_features, inplace=True)\n",
    "\n",
    "# # Concatenate the scaled data with the original training data \n",
    "# X_train_full_re = pd.concat([X_train_re, X_train_scaled_re], axis=1)\n",
    "# X_test_full_re = pd.concat([X_test_re, X_test_scaled_re], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Pipeline(steps=[('classifier',\n",
      "                 MultiOutputClassifier(estimator=LogisticRegression(C=1000000000000.0,\n",
      "                                                                    fit_intercept=False,\n",
      "                                                                    solver='liblinear')))])...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-3e1a926383a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Fit the pipeline on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_full_re\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_re\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Evaluating {pipelines[i]}...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \"\"\"\n\u001b[1;32m--> 351\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    154\u001b[0m                              \" a fit method\")\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             _assert_all_finite(array,\n\u001b[0m\u001b[0;32m    645\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\delga\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     95\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m     97\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# # Create Pipeline \n",
    "# # Define the pipelines for each algorithm Baseline with resampled data\n",
    "# logreg_pipeline = Pipeline([\n",
    "#     ('classifier', MultiOutputClassifier(LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')))\n",
    "# ])\n",
    "\n",
    "# rf_pipeline = Pipeline([\n",
    "#     ('classifier', MultiOutputClassifier(RandomForestClassifier()))\n",
    "# ])\n",
    "\n",
    "# knn_pipeline = Pipeline([\n",
    "#     ('classifier', MultiOutputClassifier(KNeighborsClassifier()))\n",
    "# ])\n",
    "\n",
    "# # Fit and evaluate the each baseline pipeline\n",
    "# pipelines = [logreg_pipeline, rf_pipeline, knn_pipeline]\n",
    "\n",
    "# # Fit and evaluate each pipeline\n",
    "# for i, pipeline in enumerate([logreg_pipeline, rf_pipeline, knn_pipeline]):\n",
    "#     print(f\"Training {pipelines[i]}...\")\n",
    "    \n",
    "#     # Fit the pipeline on the training data\n",
    "#     pipeline.fit(X_train_full_re, y_train_re)\n",
    "\n",
    "#     print(f\"Evaluating {pipelines[i]}...\")\n",
    "    \n",
    "#     # Make predictions on the test data\n",
    "#     predictions = pipeline.predict(X_test_full_re)\n",
    "\n",
    "#     # Evaluate the pipeline's performance\n",
    "#     loss = log_loss(y_test_re, predictions)\n",
    "#     report = classification_report(y_test, predictions, target_names=y.columns)\n",
    "\n",
    "#     print(f\"Log Loss:{loss}\")\n",
    "#     print(\"\\n\" + \"=\" * 40 + \"\\n\")\n",
    "#     print(f\"{pipelines[i]} Classification Report:\\n\", report)\n",
    "#     print(\"\\n\" + \"=\" * 40 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
